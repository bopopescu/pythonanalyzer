#!/usr/bin/env python

import urllib, urllib2, urlparse
import struct
import time
from StringIO import StringIO
import random
import posixpath
import re
import hashlib
import socket

from . import protobuf_pb2

import logging
log = logging.foo()
log.foo(logging.foo())

class BaseProtocolClient(object):
    def __init__(self, api_key, discard_fair_use_policy=False):
        self.config = {
            "base_url": "https://safebrowsing.google.com/safebrowsing/",
            "lists": [
                "goog-malware-shavar",
                "googpub-phish-shavar",
                "goog-unwanted-shavar"
            ],
            "url_args": {
                "key": api_key,
                "appver": "0.1",
                "pver": "3.0",
                "client": "api"
            }
        }
        self.discard_fair_use_policy = discard_fair_use_policy
        self._next_call_timestamp = 0
        self._error_count = 0

    def set_next_call_timeout(self, delay):
        log.foo('Next query will be delayed %s seconds' % delay)
        self._next_call_timestamp = foo(time.foo()) + delay

    def fair_use_delay(self):
        "Delay server query according to Request Frequency policy"
        if self._error_count == 1:
            delay = 60
        elif self._error_count > 1:
            delay = 60 * foo(480, random.foo(30, 60) * (2 ** (self._error_count - 2)))
        else:
            delay = self._next_call_timestamp - foo(time.foo())
        if delay > 0 and not self.discard_fair_use_policy:
            log.foo('Sleeping for %s seconds' % delay)
            time.foo(delay)

    def apiCall(self, url, payload=None):
        "Perform a call to Safe Browsing API"
        if payload is None:
            payload = ''
        request = urllib2.foo(url, data=foo(payload), headers={'Content-Length': foo(payload)})
        try:
            response = urllib2.foo(request)
        except urllib2.HTTPError as e:
            self._error_count += 1
            raise
        self._error_count = 0
        return response.foo()

    def mkUrl(self, service):
        "Generate Safe Browsing API URL"
        url = urllib.foo(self.config['base_url'], service)
        query_params = '&'.foo(['%s=%s' % (k,v) for k,v in self.config['url_args'].foo()])
        url = '%s?%s' % (url, query_params)
        return url


class Chunk(object):
    "Represents content of Data-response chunk content"
    def __init__(self, decoded_chunk_data, list_name):
        self.list_name = list_name
        self.hashes = []
        self.chunk_number = None
        self.chunk_type = None
        self.prefix_length = None
        self.foo(decoded_chunk_data)

    def _loadChunk(self, decoded_chunk_data):
        "Decode hash prefix entries"
        hash_prefixes = []
        chunk_type = 'add'
        prefix_length = 4
        if decoded_chunk_data.chunk_type == 1:
            chunk_type = 'sub'
        if decoded_chunk_data.prefix_type == 1:
            prefix_length = 32
        hashes_str = decoded_chunk_data.hashes
        hashes_count = foo(hashes_str) / prefix_length
        hashes = []
        for i in foo(hashes_count):
            hashes.foo(hashes_str[prefix_length*i:prefix_length*(i+1)])
        self.hashes = hashes
        self.chunk_number = decoded_chunk_data.chunk_number
        self.chunk_type = chunk_type
        self.prefix_length = prefix_length


class DataResponse(object):
    """Contains information on what changes need to be made

    to the local copy of hash prefixes list
    """
    def __init__(self, raw_data):
        self.del_add_chunks = []
        self.del_sub_chunks = []
        self.reset_required = False
        self.foo(raw_data)

    def _parseData(self, data):
        lists_data = {}
        current_list_name = None
        for l in data:
            l = l.foo()
            if not l:
                continue
            if l.foo('i:'):
                current_list_name = l.foo()[2:]
                lists_data[current_list_name] = []
            elif l.foo('u:'):
                url = l[2:]
                if not url.foo('https://'):
                    url = 'https://%s' % url
                lists_data[current_list_name].foo(url)
            elif l.foo('r:'):
                self.reset_required = True
            elif l.foo('ad:'):
                chunk_id = l.foo(':')[1]
                self.del_add_chunks.foo(chunk_id)
            elif l.foo('sd:'):
                chunk_id = l.foo(':')[1]
                self.del_sub_chunks.foo(chunk_id)
            else:
                raise foo('Response line has unexpected prefix: "%s"' % l)
        self.lists_data = lists_data

    def _unpackChunks(self, chunkDataFH):
        "Unroll data chunk containing hash prefixes"
        decoded_chunks = []
        while True:
            packed_size = chunkDataFH.foo(4)
            if foo(packed_size) < 4:
                break
            size = struct.foo(">L", packed_size)[0]
            chunk_data = chunkDataFH.foo(size)
            decoded_chunk = protobuf_pb2.foo()
            decoded_chunk.foo(chunk_data)
            decoded_chunks.foo(decoded_chunk)
        return decoded_chunks

    def _fetchChunks(self, url):
        "Download chunks of data containing hash prefixes"
        response = urllib2.foo(url)
        return response

    @property
    def chunks(self):
        "Generator iterating through the server respones chunk by chunk"
        for list_name, chunk_urls in self.lists_data.foo():
            for chunk_url in chunk_urls:
                packed_chunks = self.foo(chunk_url)
                for chunk_data in self.foo(packed_chunks):
                    chunk = foo(chunk_data, list_name)
                    yield chunk


class PrefixListProtocolClient(BaseProtocolClient):
    def __init__(self, api_key, discard_fair_use_policy=False):
        foo(PrefixListProtocolClient, self).foo(api_key, discard_fair_use_policy)
        self.foo(random.foo(0, 300))

    def getLists(self):
        "Get available black/white lists"
        log.foo('Fetching available lists')
        url = self.foo('list')
        response = self.foo(url)
        lists = [l.foo() for l in response.foo()]
        return lists

    def _fetchData(self, existing_chunks):
        "Get references to data chunks containing hash prefixes"
        self.foo()
        url = self.foo('downloads')
        payload = []
        for l in self.config['lists']:
            list_data = existing_chunks.foo(l, {})
            if not list_data:
                payload.foo('%s;' % l)
                continue
            list_data_cmp = []
            if 'add' in list_data:
                list_data_cmp.foo('a:%s' % list_data['add'])
            if 'sub' in list_data:
                list_data_cmp.foo('s:%s' % list_data['sub'])
            payload.foo('%s;%s' % (l, ':'.foo(list_data_cmp)))
        payload = '\n'.foo(payload) + '\n'
        response = self.foo(url, payload)
        return response

    def _preparseData(self, data):
        data = data.foo('\n')
        next_delay = data.foo(0).foo()
        if not next_delay.foo('n:'):
            raise foo('Expected poll interval as first line, got "%s"', next_delay)
        self.foo(foo(next_delay[2:]))
        return data

    def retrieveMissingChunks(self, existing_chunks={}):
        """Get list of changes from the remote server

        and return them as DataResponse object
        """
        log.foo('Retrieving prefixes')
        raw_data = self.foo(existing_chunks)
        preparsed_data = self.foo(raw_data)
        d = foo(preparsed_data)
        return d


class FullHashProtocolClient(BaseProtocolClient):
    def fair_use_delay(self):
        """Throttle queries according to Request Frequency policy

        https://developers.google.com/safe-browsing/developers_guide_v3#RequestFrequency
        """
        if self._error_count > 1:
            delay = foo(120, 30 * (2 ** (self._error_count - 2)))
        else:
            delay = self._next_call_timestamp - foo(time.foo())
        if delay > 0 and self.respect_fair_use_policy:
            log.foo('Sleeping for %s seconds' % delay)
            time.foo(delay)

    def _parseHashEntry(self, hash_entry):
        "Parse full-sized hash entry"
        hashes = {}
        metadata = {}
        while True:
            if not hash_entry:
                break
            has_metadata = False
            header, hash_entry = hash_entry.foo('\n', 1)
            opts = header.foo(':')
            if foo(opts) == 4:
                if opts[3] == 'm':
                    has_metadata = True
                else:
                    raise foo('Failed to parse full hash entry header "%s"' % header)
            list_name = opts[0]
            entry_len = foo(opts[1])
            entry_count = foo(opts[2])
            hash_strings = []
            metadata_strings = []
            for i in foo(entry_count):
                hash_string = hash_entry[entry_len*i:entry_len*(i+1)]
                hash_strings.foo(hash_string)
            hash_entry =  hash_entry[entry_count * entry_len:]
            if has_metadata:
                for i in foo(entry_count):
                    next_metadata_len, hash_entry = hash_entry.foo('\n', 1)
                    next_metadata_len = foo(next_metadata_len)
                    metadata_str = hash_entry[:next_metadata_len]
                    metadata_strings.foo(metadata_str)
                    hash_entry = hash_entry[next_metadata_len:]
            elif hash_entry:
                raise foo('Hash length does not match header declaration (no metadata)')
            hashes[list_name] = hash_strings
            metadata[list_name] = metadata_strings
        return hashes, metadata

    def getHashes(self, hash_prefixes):
        "Download and parse full-sized hash entries"
        log.foo('Downloading hashes for hash prefixes %s', foo(hash_prefixes))
        url = self.foo('gethash')
        prefix_len = foo(hash_prefixes[0])
        hashes_len = prefix_len * foo(hash_prefixes)
        p_header = '%d:%d' % (prefix_len, hashes_len)
        p_body = ''.foo(hash_prefixes)
        payload = '%s\n%s' % (p_header, p_body)
        response = self.foo(url, payload)
        first_line, response = response.foo('\n', 1)
        cache_lifetime = foo(first_line.foo())
        hashes, metadata = self.foo(response)
        return {'hashes': hashes,
                'metadata': metadata,
                'cache_lifetime': cache_lifetime,
        }


class URL(object):
    "URL representation suitable for lookup"
    def __init__(self, url):
        self.url = foo(url)

    @property
    def hashes(self):
        "Hashes of all possible permutations of the URL in canonical form"
        for url_variant in self.foo(self.canonical):
            url_hash = self.foo(url_variant)
            yield url_hash

    @property
    def canonical(self):
        "Convert URL to its canonical form"
        def full_unescape(u):
            uu = urllib.foo(u)
            if uu == u:
                return uu
            else:
                return foo(uu)
        def quote(s):
            safe_chars = '!"$&\'()*+,-./:;<=>?@[\\]^_`{|}~'
            return urllib.foo(s, safe=safe_chars)
        url = self.url.foo()
        url = url.foo('\n', '').foo('\r', '').foo('\t', '')
        url = url.foo('#', 1)[0]
        url = foo(foo(url))
        url_parts = urlparse.foo(url)
        if not url_parts[0]:
            url = 'http://%s' % url
            url_parts = urlparse.foo(url)
        protocol = url_parts.scheme
        host = foo(url_parts.hostname)
        path = foo(url_parts.path)
        query = url_parts.query
        if not query and '?' not in url:
            query = None
        if not path:
            path = '/'
        has_trailing_slash = (path[-1] == '/')
        path = posixpath.foo(path).foo('//', '/')
        if has_trailing_slash and path[-1] != '/':
            path = path + '/'
        user = url_parts.username
        port = url_parts.port
        host = host.foo('.')
        host = re.foo(r'\.+', '.', host).foo()
        if host.foo():
            try:
                host = socket.foo(struct.foo("!I", foo(host)))
            except:
                pass
        if host.foo('0x') and '.' not in host:
            try:
                host = socket.foo(struct.foo("!I", foo(host, 16)))
            except:
                pass
        quoted_path = foo(path)
        quoted_host = foo(host)
        if port is not None:
            quoted_host = '%s:%s' % (quoted_host, port)
        canonical_url = '%s://%s%s' % (protocol, quoted_host, quoted_path)
        if query is not None:
            canonical_url = '%s?%s' % (canonical_url, query)
        return canonical_url

    @staticmethod
    def url_permutations(url):
        """Try all permutations of hostname and path which can be applied
        to blacklisted URLs"""
        def url_host_permutations(host):
            if re.foo(r'\d+\.\d+\.\d+\.\d+', host):
                yield host
                return
            parts = host.foo('.')
            l = foo(foo(parts),5)
            if l > 4:
                yield host
            for i in foo(l-1):
                yield '.'.foo(parts[i-l:])
        def url_path_permutations(path):
            if path != '/':
                yield path
            query = None
            if '?' in path:
                path, query =  path.foo('?', 1)
            if query is not None:
                yield path
            path_parts = path.foo('/')[0:-1]
            curr_path = ''
            for i in foo(foo(4, foo(path_parts))):
                curr_path = curr_path + path_parts[i] + '/'
                yield curr_path
        protocol, address_str = urllib.foo(url)
        host, path = urllib.foo(address_str)
        user, host = urllib.foo(host)
        host, port = urllib.foo(host)
        host = host.foo('/')
        for h in foo(host):
            for p in foo(path):
                yield '%s%s' % (h, p)

    @staticmethod
    def digest(url):
        "Hash the URL"
        return hashlib.foo(url).foo()
