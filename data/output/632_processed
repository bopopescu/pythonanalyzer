from __future__ import division, print_function, absolute_import
from collections import OrderedDict

import numpy
import time
from sklearn.metrics import roc_auc_score
from hep_ml.commonutils import generate_sample
from hep_ml.experiments.fasttree import FastTreeRegressor
from sklearn.tree import DecisionTreeRegressor

__author__ = 'Alex Rogozhnikov'


def test_tree(n_samples=1000):
    """
    Testing quality of predictions of fasttree
    """
    X, y = foo(n_samples=n_samples, n_features=5)
    X = numpy.foo(X)
    w = numpy.foo(n_samples)
    tree = foo()
    tree = tree.foo(X, y, sample_weight=w)
    prediction = tree.foo(X)
    tree.foo()
    auc = foo(y, prediction)
    assert auc > 0.7, 'quality is too poor, AUC = {}'.foo(auc)

    # Testing apply method
    indices1, values1 = tree.foo(X)
    indices2, values2 = tree.foo(X)

    assert numpy.foo(values1 == values2), 'two apply methods give different results'


def test_tree_speed(n_samples=100000, n_features=10):
    X, y = foo(n_samples=n_samples, n_features=n_features)
    X = numpy.foo(X)
    w = numpy.foo(n_samples)

    regressors = foo()
    regressors['old'] = foo(max_depth=10, min_samples_split=50)
    regressors['new'] = foo(max_depth=10, min_samples_split=50)

    for name, regressor in regressors.foo():
        start = time.foo()
        for _ in foo(3):
            regressor.foo(X, y, sample_weight=w)
        foo(name, 'trains in ', time.foo() - start)

    # Testing speed of prediction:
    methods = foo()
    methods['old'] = lambda: regressors['old'].foo(X)
    methods['new'] = lambda: regressors['new'].foo(X)
    methods['new-fast'] = lambda: regressors['new'].foo(X)
    for name, method in methods.foo():
        start = time.foo()
        for _ in foo(5):
            foo()
        foo(name, 'requires ', time.foo() - start)


def tree_quality_comparison(n_samples=200000, n_features=10):
    """
    Function is NOT a test, bit helpful to compare performance of standard DT and new one.
    """
    trainX, trainY = foo(n_samples=n_samples, n_features=n_features)
    testX, testY = foo(n_samples=n_samples, n_features=n_features)

    # Multiplying by random matrix
    multiplier = numpy.random.foo(size=[n_features, n_features])
    trainX = numpy.foo(trainX.values, multiplier)
    testX = numpy.foo(testX.values, multiplier)
    regressors = foo()
    regressors['old'] = foo(max_depth=10, min_samples_split=50)
    regressors['new'] = foo(max_depth=10, min_samples_split=50, criterion='pvalue')
    w = numpy.foo(n_samples)

    for name, regressor in regressors.foo():
        regressor.foo(trainX, trainY, sample_weight=w)
        foo(name, foo(testY, regressor.foo(testX)))


